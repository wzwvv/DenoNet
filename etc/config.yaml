
# Parameters for training procedure
train_param:
  datasets: 2        #数据集选择
  pre_train: False
  ratio: 0.85          #训练集占比
  ws: 1.0                 #窗口长度F
  Kf: 5
  bz: 36
  epochs: 300
  testmethod: FBCCA
  noisetype: all #time,space,freq,random，all
  noise_ratio: 0.25 #0,0.25,0.5,0.75,1
  snr_db: 5  #10,5,0
  smooth: True
  cuda : 0
  savefilename: resultsBASLINEcnn.xlsx #resultssen.xlsx 

model_param:
    Nm: 3
    Bandpass: False
    KLG: True         #知识层
    MIX: false
    DL: True                #深度学习层
    FB: True
    Es: 1

gan_model_param:
  lambda_G: 1   # 控制G分类损失
  lambda_D: 1    # 控制D分类损失
  lambda_vae: 1    #重建回归损失
  lambda_kl: 0.1   #重建KL散度损失
  lr_G: 0.001
  lr_D: 0.001
  wd_D: 0.0001
  wd_G: 0.0001
  lr_jitter: true


vae_model_param:
  lr: 0.001
  wd: 0.0001
  lambda_vae: 1
  lr_jitter: true
  Nm: 3

# Parameters for ssvep data
data_param1:
  Nh: 180                      # number of trial
  Nc: 8                        # number of channel
  Fs: 256                      # frequency of sample
  Nf: 12                       # number of stimulus
  Ns: 10                       # number of subjects
  initfreq: 9.25
  deltafreq: 0.5
  Nt: 1024

data_param2:
  Nh: 240                      # number of trial
  Nc: 9                      # number of channel
  Fs: 250                      # frequency of sample
  Nf: 40                       # number of stimulus
  Ns: 35
  initfreq: 8
  deltafreq: 0.2
  Nt: 1500

data_param3:
  Nh: 160                      # number of trial
  Nc: 64                      # number of channel
  Fs: 250                      # frequency of sample
  Nf: 40                       # number of stimulus
  Ns: 70
  initfreq: 8
  deltafreq: 0.2
  Nt: 750


slclayer_param:
  initweight: 1           #其他通道初始权重
  SLC_FIX: True        #通道选择层参数固定

TRCA:
  slc_all: False
  is_ensemble: False
  n_components: 1
  Bandpass_FIX: False
  KLG_FIX: False
  FB_FIX: False

TDCA:
  slc_all: True
  lagging_len: 5
  n_components: 3
  Bandpass_FIX: False
  KLG_FIX: False
  FB_FIX: False

TD-RCA:
  slc_all: True
  lagging_len: 5
  n_components: 3
  is_ensemble: False
  Bandpass_FIX: False
  KLG_FIX: False
  FB_FIX: False


MIXlayer:
  mix_FIX: False

# Parameters for DL-based methods
EEGNet:
  epochs: 300                  # number of epochs
  bz: 30                     # batch size
  lr: 0.005                       # learning rate
  wd: 0.0001                   # weight decay
  lr_jitter: false             # learning rate scheduler

CCNN:
  epochs: 300                  # number of epochs
  bz: 30                     # batch size
  lr: 0.001                       # learning rate
  wd: 0.0001                   # weight decay
  lr_jitter: false             # learning rate scheduler

FBtCNN:
  epochs: 500                  # number of epochs
  bz: 30                     # batch size
  lr: 0.001                       # learning rate
  wd: 0.01                   # weight decay
  lr_jitter: false             # learning rate scheduler


ConvCA:
  epochs: 1000                  # number of epochs
  bz: 30                     # batch size
  lr: 0.001                       # learning rate
  wd: 0.0000                  # weight decay
  lr_jitter: false             # learning rate scheduler

SSVEPNet:
  epochs: 400                  # number of epochs
  bz: 30                    # batch size
  lr: 0.01                       # learning rate
  wd: 0.0003                   # weight decay
  lr_jitter: true             # learning rate scheduler
  stimulus_type: 12           # 4-class or 12-class

SSVEPformer:
  epochs: 100                # number of epochs
  bz: 30                     # batch size
  lr: 0.001                       # learning rate
  wd: 0.0001                   # weight decay
  lr_jitter: True             # learning rate scheduler

DDGCNN:
  epochs: 400                  # number of epochs
  bz: 30                     # batch size
  lr: 0.001                   # learning rate
  wd: 0.0001                   # weight decay
  lr_jitter: true             # learning rate scheduler
  lr_decay_rate: 0.75         # learning rate decay rate
  optim_patience: 300        # optimizer patience
  trans_class: DCD           # {DCD, linear, normal_conv}
  act: leakyrelu             # activation layer {relu, prelu, leakyrelu}
  norm: layer                # {batch, layer, instance} normalization
  n_filters: 128            # 64 or 128









